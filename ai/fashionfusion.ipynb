{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T01:00:28.888356Z",
     "iopub.status.busy": "2025-03-15T01:00:28.887949Z",
     "iopub.status.idle": "2025-03-15T01:00:34.692854Z",
     "shell.execute_reply": "2025-03-15T01:00:34.691922Z",
     "shell.execute_reply.started": "2025-03-15T01:00:28.888309Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T01:00:34.694387Z",
     "iopub.status.busy": "2025-03-15T01:00:34.693962Z",
     "iopub.status.idle": "2025-03-15T01:00:34.975242Z",
     "shell.execute_reply": "2025-03-15T01:00:34.974522Z",
     "shell.execute_reply.started": "2025-03-15T01:00:34.694362Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the image folder exist? True\n",
      "Sample image files: ['31973.jpg', '30778.jpg', '19812.jpg', '22735.jpg', '38246.jpg', '16916.jpg', '52876.jpg', '39500.jpg', '44758.jpg', '59454.jpg']\n"
     ]
    }
   ],
   "source": [
    "#csv_file = \"/kaggle/input/fashion-product-images-dataset/fashion-dataset/sty.csv\"\n",
    "\n",
    "#df = pd.read_csv(csv_file, on_bad_lines=\"skip\")\n",
    "#print(df.head())  # Show the first few rows\n",
    "#print(df.columns)  # Display all column names\n",
    "\n",
    "\n",
    "root_dir = \"/kaggle/input/fashion-product-images-dataset/fashion-dataset\"\n",
    "image_folder = \"images\"  # Update if necessary\n",
    "\n",
    "# Check if the image folder exists\n",
    "print(\"Does the image folder exist?\", os.path.exists(os.path.join(root_dir, image_folder)))\n",
    "\n",
    "# List some images\n",
    "sample_images = os.listdir(os.path.join(root_dir, image_folder))[:10]\n",
    "print(\"Sample image files:\", sample_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T01:00:34.977053Z",
     "iopub.status.busy": "2025-03-15T01:00:34.976704Z",
     "iopub.status.idle": "2025-03-15T01:00:34.983820Z",
     "shell.execute_reply": "2025-03-15T01:00:34.983058Z",
     "shell.execute_reply.started": "2025-03-15T01:00:34.977021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ClothesDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file, on_bad_lines='skip')  # Skips bad lines\n",
    "        #self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.categories = sorted(self.data_frame[\"articleType\"].unique())\n",
    "        self.category_to_idx = {cat: idx for idx, cat in enumerate(self.categories)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, \"images\", str(self.data_frame.iloc[idx][\"id\"]) + \".jpg\")\n",
    "        \n",
    "        if not os.path.exists(img_name):\n",
    "             print(f\"Warning: Image {img_name} not found. Skipping it.\")\n",
    "             return torch.zeros((3, 224, 224)), torch.tensor(-1, dtype=torch.long)  # Fix: Correct tensor creation\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        label = self.category_to_idx[self.data_frame.iloc[idx][\"articleType\"]]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T01:00:34.985129Z",
     "iopub.status.busy": "2025-03-15T01:00:34.984898Z",
     "iopub.status.idle": "2025-03-15T01:00:34.999085Z",
     "shell.execute_reply": "2025-03-15T01:00:34.998212Z",
     "shell.execute_reply.started": "2025-03-15T01:00:34.985109Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T01:00:35.000209Z",
     "iopub.status.busy": "2025-03-15T01:00:34.999911Z",
     "iopub.status.idle": "2025-03-15T01:00:35.153036Z",
     "shell.execute_reply": "2025-03-15T01:00:35.152191Z",
     "shell.execute_reply.started": "2025-03-15T01:00:35.000180Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = ClothesDataset(csv_file=\"/kaggle/input/fashion-product-images-dataset/fashion-dataset/styles.csv\", root_dir=\"/kaggle/input/fashion-product-images-dataset/fashion-dataset\", transform=transform)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T01:00:35.154078Z",
     "iopub.status.busy": "2025-03-15T01:00:35.153813Z",
     "iopub.status.idle": "2025-03-15T01:00:36.301337Z",
     "shell.execute_reply": "2025-03-15T01:00:36.300568Z",
     "shell.execute_reply.started": "2025-03-15T01:00:35.154056Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 190MB/s]\n"
     ]
    }
   ],
   "source": [
    "class ClothesClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ClothesClassifier, self).__init__()\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "num_classes = len(dataset.categories)\n",
    "model = ClothesClassifier(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T01:00:36.302515Z",
     "iopub.status.busy": "2025-03-15T01:00:36.302191Z",
     "iopub.status.idle": "2025-03-15T01:00:36.307560Z",
     "shell.execute_reply": "2025-03-15T01:00:36.306477Z",
     "shell.execute_reply.started": "2025-03-15T01:00:36.302474Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T01:01:32.197753Z",
     "iopub.status.busy": "2025-03-15T01:01:32.197425Z",
     "iopub.status.idle": "2025-03-15T01:11:56.024909Z",
     "shell.execute_reply": "2025-03-15T01:11:56.023654Z",
     "shell.execute_reply.started": "2025-03-15T01:01:32.197727Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Warning: Image /kaggle/input/fashion-product-images-dataset/fashion-dataset/images/12347.jpg not found. Skipping it.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target -1 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-62eeda79f272>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m         return F.cross_entropy(\n\u001b[0m\u001b[1;32m   1294\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3479\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3480\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3481\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target -1 is out of bounds."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "num_epochs = 10  # Define number of epochs\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    running_loss = 0.0\n",
    "    print(1)\n",
    "    # Training Loop\n",
    "    for images, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "        running_loss += loss.item()\n",
    "    print(2)\n",
    "    # Calculate train loss and accuracy\n",
    "    train_loss = running_loss / len(dataloader)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Validation Step\n",
    "    model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    val_running_loss = 0.0\n",
    "    print(3)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "            val_running_loss += loss.item()\n",
    "    print(4)\n",
    "    # Calculate validation loss and accuracy\n",
    "    val_loss = val_running_loss / len(val_dataloader)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    print(5)\n",
    "    # Print results for each epoch\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_accuracy:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-15T01:01:19.628167Z",
     "iopub.status.idle": "2025-03-15T01:01:19.628421Z",
     "shell.execute_reply": "2025-03-15T01:01:19.628317Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot Training & Validation Loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(np.arange(1, num_epochs+1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(np.arange(1, num_epochs+1), val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Training & Validation Accuracy\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(np.arange(1, num_epochs+1), train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(np.arange(1, num_epochs+1), val_accuracies, label=\"Val Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Training & Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-15T01:01:19.629099Z",
     "iopub.status.idle": "2025-03-15T01:01:19.629405Z",
     "shell.execute_reply": "2025-03-15T01:01:19.629294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"clothes_classifier.pth\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-15T01:01:19.629914Z",
     "iopub.status.idle": "2025-03-15T01:01:19.630193Z",
     "shell.execute_reply": "2025-03-15T01:01:19.630087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#testing model\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from torchvision import models\n",
    "\n",
    "# Define paths (model should be in the same directory)\n",
    "model_path = \"clothes_classifier.pth\"  # Adjust this if your model has a different name\n",
    "csv_path = \"styles.csv\"  # CSV file in the same directory\n",
    "\n",
    "# Check if the model exists\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model file '{model_path}' not found. Make sure it's saved in the same directory.\")\n",
    "\n",
    "# Define the number of categories (match this with training)\n",
    "num_classes = 50  # Change this if your dataset has a different number of categories\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model architecture\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-15T01:01:19.631298Z",
     "iopub.status.idle": "2025-03-15T01:01:19.631594Z",
     "shell.execute_reply": "2025-03-15T01:01:19.631482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define image transformations (must match training preprocessing)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize([0.5], [0.5])  \n",
    "])\n",
    "\n",
    "# Load and preprocess your image\n",
    "image_path = \"/kaggle/input/testing-images/1.png\"  # Change this to your test image\n",
    "if not os.path.exists(image_path):\n",
    "    raise FileNotFoundError(f\"Test image '{image_path}' not found!\")\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "image = image.to(device)\n",
    "\n",
    "print(\"âœ… Image preprocessed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-15T01:01:19.632312Z",
     "iopub.status.idle": "2025-03-15T01:01:19.632597Z",
     "shell.execute_reply": "2025-03-15T01:01:19.632468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run inference on the image\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "\n",
    "predicted_index = predicted.item()\n",
    "print(f\"Predicted class index: {predicted_index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-15T01:01:19.633467Z",
     "iopub.status.idle": "2025-03-15T01:01:19.633735Z",
     "shell.execute_reply": "2025-03-15T01:01:19.633628Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Ensure column names match (modify if necessary)\n",
    "category_list = sorted(df[\"subCategory\"].dropna().unique())  \n",
    "color_list = sorted(df[\"baseColour\"].dropna().unique())  \n",
    "\n",
    "# Get the predicted category and color\n",
    "predicted_category = category_list[predicted_index % len(category_list)]\n",
    "predicted_color = color_list[predicted_index % len(color_list)]\n",
    "\n",
    "print(f\"ðŸŽ¯ Predicted Category: {predicted_category}\")\n",
    "print(f\"ðŸŽ¨ Predicted Color: {predicted_color}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 139630,
     "sourceId": 329006,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6873025,
     "sourceId": 11034817,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
